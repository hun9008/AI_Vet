/**
* This code was generated by v0 by Vercel.
* @see https://v0.dev/t/IBsIDx9UuxJ
* Documentation: https://v0.dev/docs#integrating-generated-code-into-your-nextjs-app
*/

/** Add fonts into your Next.js project:

import { IBM_Plex_Sans } from 'next/font/google'
import { Gabarito } from 'next/font/google'

ibm_plex_sans({
  subsets: ['latin'],
  display: 'swap',
})

gabarito({
  subsets: ['latin'],
  display: 'swap',
})

To read more about using these font, please visit the Next.js documentation:
- App Directory: https://nextjs.org/docs/app/building-your-application/optimizing/fonts
- Pages Directory: https://nextjs.org/docs/pages/building-your-application/optimizing/fonts
**/
"use client";

import React, { useState, useCallback } from 'react';
import { useDropzone } from 'react-dropzone';
import axios from 'axios';
// import Image from 'next/image';
import Link from "next/link";
import { Avatar, AvatarImage, AvatarFallback } from "@/components/ui/avatar";
import Button from "@/components/ui/button";
import { Prism as SyntaxHighlighter } from 'react-syntax-highlighter';
import { darcula } from 'react-syntax-highlighter/dist/esm/styles/prism';


export function Cataract() {
  const [uploadedImage, setUploadedImage] = useState<string | null>(null);
  const [processedImages, setProcessedImages] = useState<string[]>([]);
  const [predictedClass, setPredictedClass] = useState<string | null>(null);
  const [probability, setProbability] = useState<number | null>(null);
  const [allProbability, setAllProbability] = useState<number[][] | null>(null);
  const [activeCode, setActiveCode] = useState('model');
  const [loading, setLoading] = useState(false);
  

  const onDrop = useCallback(async (acceptedFiles: File[]) => {
    const file = acceptedFiles[0];
    const reader = new FileReader();
    reader.onloadend = async () => {
      let base64 = reader.result as string;
      // console.log(base64)

      const base64Index = base64.indexOf(',') + 1;
      base64 = base64.substring(base64Index)
      console.log(base64)

      setUploadedImage(base64);
      setLoading(true);

      try {
        const header = {
          "Content-Type": "application/json",
        };
        const body = {
          "img":base64,
        }

        const response = await axios.post('https://testmodel.hunian.site/inference/test', body, { headers: header });

        const { predicted_class, probability, all_probability, lime, vit } = response.data;
        setPredictedClass(predicted_class);
        setProbability(probability);
        setAllProbability(all_probability);
        setProcessedImages([lime, vit]);
      } catch (error) {
        console.error('Error uploading image:', error);
      } finally {
        setLoading(false);
      }
    };
    reader.readAsDataURL(file);
  }, []);

  const getKoreanClass = (predictedClass: string | null) => {
    switch (predictedClass) {
      case 'incipient':
        return '초기 (incipient)';
      case 'mature':
        return '성숙 (mature)';
      case 'overripe':
        return '과숙 (overripe)';
      case 'no':
        return '정상 (normal)';
      default:
        return '';
    }
  };

  const { getRootProps, getInputProps } = useDropzone({ onDrop });

  return (
    <div className="flex flex-col min-h-dvh">
      <header className="bg-primary text-primary-foreground px-4 lg:px-6 py-6 flex items-center justify-between">
        <div>
          <h1 className="text-3xl font-bold">AI Vet</h1>
          <p className="text-sm text-primary-foreground/80">Detect and monitor pet cataracts with AI</p>
        </div>
        <div className="ml-auto flex items-center gap-2">
          <Link
            href="https://github.com/hun9008/Dev_Cataract"
            className="inline-flex h-9 items-center justify-center rounded-md bg-gray-800 px-4 py-2 text-sm font-medium text-white shadow transition-colors hover:bg-gray-900 focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50"
            prefetch={false}
          >
            GitHub
          </Link>
          <Link
            href="https://drive.google.com/drive/folders/1NHui2kw6M3--jb8fTyore6kN9xYU32HW?usp=sharing"
            className="inline-flex h-9 items-center justify-center rounded-md bg-primary-foreground px-4 py-2 text-sm font-medium text-primary shadow transition-colors hover:bg-primary-foreground/90 focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50"
            prefetch={false}
          >
            Download App
          </Link>
        </div>
      </header>
      <main className="flex-1">
        <section className="py-12 md:py-24 lg:py-32 bg-muted flex justify-center items-center">
          <div className="container px-4 md:px-6 grid gap-8 lg:grid-cols-2 lg:gap-12">
            <div className="space-y-4">
              <h2 className="text-3xl font-bold tracking-tighter sm:text-4xl md:text-5xl">
                Detect and Monitor Pet Cataracts
              </h2>
              <p className="text-muted-foreground md:text-xl">
              AI 기반 앱은 반려동물의 백내장을 쉽게 감지하고 모니터링하는 데 도움이 됩니다. 조기 진단을 받고
              상태의 진행 상황을 추적하세요.
              </p>
              <div className="flex justify-between">
                <img src={'/roc_curve.png'} width={300} height={240} alt="ROC Curve" />
                <img src={'/confusion_matrix.png'} width={300} height={240} alt="Confusion Matrix" />
              </div>
            </div>
            <div className="grid gap-6">
              <div className="flex items-start gap-4">
                <Avatar className="shrink-0">
                  <AvatarImage src="/placeholder-user.jpg" alt="Dr. Emily Johnson" />
                  <AvatarFallback>YH</AvatarFallback>
                </Avatar>
                <div>
                  <h3 className="text-lg font-semibold">정 용훈</h3>
                  <p className="text-muted-foreground">BE & AI Engineer, Team Leader</p>
                  <p className="text-sm text-muted-foreground">
                  백엔드 개발과 AI 모델링의 주요 책임을 맡고 있습니다. 
                  팀의 전반적인 기술 전략을 수립하고, 
                  애플리케이션의 핵심 AI 기능을 설계 및 구현합니다.
                  </p>
                </div>
              </div>
              <div className="flex items-start gap-4">
                <Avatar className="shrink-0">
                  <AvatarImage src="/placeholder-user.jpg" alt="John Doe" />
                  <AvatarFallback>HY</AvatarFallback>
                </Avatar>
                <div>
                  <h3 className="text-lg font-semibold">남 현원</h3>
                  <p className="text-muted-foreground">BE & AI Engineer</p>
                  <p className="text-sm text-muted-foreground">
                  데이터 전처리와 서버 아키텍처 최적화 작업을 주로 담당합니다. 안정적이고 확장 가능한 백엔드 시스템을 구축하여, 애플리케이션의 안정성을 보장합니다.
                  </p>
                </div>
              </div>
              <div className="flex items-start gap-4">
                <Avatar className="shrink-0">
                  <AvatarImage src="/placeholder-user.jpg" alt="Jane Smith" />
                  <AvatarFallback>HS</AvatarFallback>
                </Avatar>
                <div>
                  <h3 className="text-lg font-semibold">김 혜성</h3>
                  <p className="text-muted-foreground">FE & AI Engineer</p>
                  <p className="text-sm text-muted-foreground">
                  사용자 인터페이스와 사용자 경험 설계를 주로 담당하는 프론트엔드 엔지니어입니다. UI 디자인과 프론트엔드 개발을 통해, 사용자에게 직관적이고 매끄러운 경험을 제공합니다.
                  </p>
                </div>
              </div>
              <div className="flex items-start gap-4">
                <Avatar className="shrink-0">
                  <AvatarImage src="/placeholder-user.jpg" alt="Michael Lee" />
                  <AvatarFallback>SB</AvatarFallback>
                </Avatar>
                <div>
                  <h3 className="text-lg font-semibold">강 수빈</h3>
                  <p className="text-muted-foreground">FE & AI Engineer</p>
                  <p className="text-sm text-muted-foreground">
                  앱 기능 구현을 주로 담당하는 프론트엔드 엔지니어입니다. 사용자 친화적인 인터페이스를 설계하는 동시에, AI 기술을 접목하여 앱의 다양한 기능들을 효과적으로 구현하는 데 주력하고 있습니다.
                  </p>
                </div>
              </div>
            </div>
          </div>
        </section>

        <section className="py-12 md:py-24 lg:py-32 flex justify-center items-center">
          <div className="container px-4 md:px-6 grid gap-8">
            <div className="flex flex-col items-center justify-center">
              <h2 className="text-3xl font-bold tracking-tighter sm:text-4xl md:text-5xl">Experience the App</h2>
              <p className="text-muted-foreground md:text-xl">
              AI Vet의 AI를 체험해보세요. (강아지 눈 사진을 넣어보세요!)
              </p>
            </div>
            <div className="grid gap-4 justify-center">
              <div {...getRootProps()} className="border-dashed border-2 border-gray-400 p-4 text-center">
                <input {...getInputProps()} />
                <p>Drag & drop an image here, or click to select one</p>
              </div>
              {loading ? ( // 로딩 중일 때 로딩 바를 표시
                <div className="flex justify-center items-center">
                  <div className="loader"></div>
                </div>
              ) : (
                <div className="flex gap-4 flex-col items-center"> {/* 여기에 flex-col을 추가하여 세로로 배치 */}
                  <div className="flex gap-4"> {/* 여기에 flex를 추가하여 가로로 배치 */}
                    {uploadedImage && (
                      <div>
                        <img
                          src={`data:image/jpeg;base64,${uploadedImage}`}
                          width={224}
                          height={224}
                          alt="Uploaded Image"
                          className="rounded-lg object-cover"
                          style={{ aspectRatio: "800/600", objectFit: "cover" }}
                        />
                      </div>
                    )}
                    {processedImages.map((img, index) => (
                      <img
                        key={index}
                        src={`data:image/jpeg;base64,${img}`}
                        width={224}
                        height={224}
                        alt={`Processed Image ${index + 1}`}
                        className="rounded-lg object-cover"
                        style={{ aspectRatio: "800/600", objectFit: "cover" }}
                      />
                    ))}
                  </div>
                  {predictedClass && probability !== null && (
                    <div className="border-2 border-gray-400 p-4 text-center">
                      <p>{`${probability.toFixed(2)}% 확률로 ${getKoreanClass(predictedClass)} 단계로 예측됩니다.`}</p>
                      {predictedClass !== 'no' && (
                        <p>가까운 병원에 방문해주세요!</p>
                      )}
                    </div>
                  )}
                </div>
              )}
            </div>
          </div>
        </section>

        <section className="py-12 md:py-24 lg:py-32 flex justify-center items-center">
          <div className="container px-4 md:px-6 flex flex-col items-center">
            <h2 className="text-3xl font-bold tracking-tighter sm:text-4xl md:text-5xl">Explore Our Codebase</h2>
            <p className="text-muted-foreground md:text-xl">AI Vet의 핵심 Code를 확인해보세요.</p>
            <div className="flex gap-4 mt-8">
            <Button
              variant="outline"
              className="hover:bg-gray-200"
              onClick={() => setActiveCode('frontend')}
            >
              Front End
            </Button>
            <Button
              variant="outline"
              className="hover:bg-gray-200"
              onClick={() => setActiveCode('backend')}
            >
              Back End
            </Button>
            <Button
              variant="outline"
              className="hover:bg-gray-200"
              onClick={() => setActiveCode('model')}
            >
              ML Model
            </Button>
            </div>
            {activeCode === 'frontend' && (
              <div className="bg-muted rounded-lg p-6 mt-8 w-full max-w-4xl">
                <SyntaxHighlighter language="javascript" style={darcula}>
                  {`
    /* Front End Code */
    const takePicture = async () => {
    if (cameraRef.current) {
      const options = { quality: 1, base64: true, exif: false };
      const photo = await cameraRef.current.takePictureAsync(options);
      setPhoto(photo);

      let dataToSend = { img:photo.base64 };

      fetch('http://modelserver_url', {
        method: 'POST',
        body: JSON.stringify(dataToSend),
        headers: {
          'Content-Type': 'application/json',
        },
      })
        .then((response) => response.json())
        .then((responseJson) => {
          console.log(responseJson);
          navigation.navigate('CameraResult', 
          {Lime: responseJson.lime,  
          Vit : responseJson.vit, 
          Predict: responseJson.predicted_class, 
          Probability: responseJson.probability});
        })
        .catch((error) => {
          console.error(error);
        });
    }
  };`}
                </SyntaxHighlighter>
              </div>
            )}
            {activeCode === 'backend' && (
              <div className="bg-muted rounded-lg p-6 mt-8 w-full max-w-4xl">
                <SyntaxHighlighter language="python" style={darcula}>
                  {`
# Back End Code
@router.post("/posting/feed")
async def post_feed(post: Post, user_id : str, predict_id : str, pet_name : str):
    user_record = collection_name_user.find_one({"_id" : ObjectId(user_id)})
    image_ids = []
    for image_data in post.image:
        image_record = {
            "filename": image_data.filename,
            "image_encoded": image_data.image_encoded
        }
        collection_name_image.insert_one(image_record)
        image_ids.append(image_record)
    for pet in user_record["pet"]:
        if str(pet["p_name"]) == pet_name:
            pet_data = {
                "p_name": pet["p_name"],
                "p_type": pet["p_type"],
                "p_color": pet["p_color"],
                "p_age": pet["p_age"],
                "profile_image": pet["profile_image"] if pet["profile_image"] else None
            }
            for predict in pet["predict"]:
                if str(predict["_id"]) == predict_id:
                    predict_data = {
                        "_id" : ObjectId(predict_id),
                        "predicted_class": predict["predicted_class"],
                        "probability": predict["probability"],
                        "all_probability": predict["all_probability"],
                        "lime" : predict["lime"],
                        "GradCam" : predict["GradCam"],
                        "date" : predict["date"]
                    }
    post_data = {
        "po_detail": post.po_detail,
        "user_id": user_record["_id"],
        "type" : user_record["type"],
        "image": image_ids,
        "pet" : pet_data,
        "final_predict" : predict_data
    }
    inserted_id = collection_name_post.insert_one(post_data).inserted_id
    return {"_id": str(inserted_id)}
                `}
                </SyntaxHighlighter>
              </div>
            )}
            {activeCode === 'model' && (
              <div className="bg-muted rounded-lg p-6 mt-8 w-full max-w-4xl">
                <SyntaxHighlighter language="python" style={darcula}>
                  {`
  # Model Code
  def vit_inference(encoding_img : str) -> dict:
    img = Image.open(io.BytesIO(base64.b64decode(encoding_img)))
    data = np.array(img)

    # Preprocessing code...

    vit_model = vit.vit_b32(
        image_size = 224,
        activation = 'softmax',
        pretrained = True,
        include_top = False,
        pretrained_top = False,
        classes = 4)

    model = tf.keras.Sequential([
            vit_model,
            tf.keras.layers.Flatten(),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dense(11, activation = tfa.activations.gelu),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dense(4, 'softmax')
        ],
        name = 'vision_transformer')
    learning_rate = 1e-4

    optimizer = tfa.optimizers.RectifiedAdam(learning_rate=learning_rate)

    model.compile(optimizer = optimizer,
                    loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2),
                    metrics = ['accuracy'])
    
    model_path = './models/weights/ViT.h5'
    model.load_weights(model_path)
    
    vit_result = vit_grad_cam(img, 0)
    vit_result = Image.fromarray(vit_result)
    buffer = io.BytesIO()
    vit_result.save(buffer, format="PNG")
    vit_encoding_img = base64.b64encode(buffer.getvalue()).decode('utf-8')

    blurred = np.expand_dims(blurred, axis=0)
    lime_image = blurred
    blurred = np.reshape(blurred, (-1, 224, 224, 3))
    predictions = model.predict(blurred)
    predicted_class_num = int(np.argmax(predictions[0]))
    confidence = float(np.max(predictions[0]))

    classes = ['overripe', 'no', 'mature', 'incipient']

    predicted_class = classes[predicted_class_num]
    grayed_lime = lime_image[0]
    lime_output = visualize_image(grayed_lime, model)
`}
                </SyntaxHighlighter>
              </div>
            )}
          </div>
        </section>
      </main>

      <footer className="bg-muted text-muted-foreground px-4 lg:px-6 py-6 flex items-center justify-between">
        <p className="text-sm">&copy; 2024 Pet Vision. All rights reserved.</p>
        <div className="flex items-center gap-4">
          <Link href="#" className="text-sm hover:underline" prefetch={false}>
            Privacy Policy
          </Link>
          <Link href="#" className="text-sm hover:underline" prefetch={false}>
            Terms of Service
          </Link>
        </div>
      </footer>
    </div>
  )
}
